import re
import os
import io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from matplotlib.figure import Figure
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, matthews_corrcoef

# Highest Expression Scores (demo round)

# Upload, filter, and organize data
db = pd.read_csv("C:/goutam/soham/MSKCC/project/database_final.csv")
db.drop(db.columns[[0,1,4,5,6,7,8,9,10,11,12,13,14]], axis=1, inplace=True)

for ind in db.index:
    y = db["Signal Sequence"][ind]
    y = y[10:]
    db["Signal Sequence"][ind] = y
print(db)

X = pd.DataFrame([ProteinAnalysis(i).count_amino_acids() for i in db['Signal Sequence']])
y = db["Expressions Scores (Highest)"]

for ind in y.index:
    if y[ind] >= y.median():
        y[ind] = 1.0
    else:
        y[ind] = 0.0

from sklearn.feature_selection import VarianceThreshold
fs = VarianceThreshold(threshold=0.1)
fs.fit_transform(X)
X2 = X.loc[:, fs.get_support()]

# Split using sklearn package
X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state = 42, stratify=y)

# train Lazy Classifier
import lazypredict
from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=matthews_corrcoef)
models_train, predictions_train = clf.fit(X_train, X_train, y_train, y_train)
models_test, predictions_test = clf.fit(X_train, X_test, y_train, y_test)
print(models_test) # output Lazy Classifier table

plt.figure(figsize=(5, 10))
sns.set_theme(style="whitegrid")
ax = sns.barplot(y=models_train.index, x="Accuracy", data= models_test)
ax.set(xlim=(0,1))
plt.show() # output Lazy Classifier bar graph

# Random Forest Classifier Model
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500) # ideal default value of n_estimators
rf.fit(X_train, y_train)
y_train_RF_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)
print(rf.score(X_test, y_test)) # output Random Forest Classifier score (pre hyper-parameter tuning)

# Enter hyper-parameter tuning phase
max_features_range = np.arange(1,6,1)
n_estimators_range = np.arange(10,210,10)
param_grid = dict(max_features = max_features_range, n_estimators = n_estimators_range)

rf = RandomForestClassifier()
grid = GridSearchCV(estimator = rf, param_grid=param_grid, cv=5)
grid.fit(X_train, y_train)
print("The best parameters are %s with a score of %0.2f" % (grid.best_params_, grid.best_score_))

# Random Forest Classifier (retrained post tuning)
rf = RandomForestClassifier(max_features = "grid.best_param_features", n_estimators="grid.best_param_estimator")
rf.fit(X_train, y_train)
y_train_pred = rf.predict(X_train)
y_test_pred_new = rf.predict(X_test)
print(rf.score(X_test, y_test)) # output Random Forest Classifier score (post hyper-parameter tuning)

from matplotlib.pyplot import rcParams
rcParams['figure.figsize'] = 10,10
from sklearn.metrics import plot_roc_curve
plot_roc_curve(rf, X_test, y_test)
plt.show()

# plot feature importance using Random Forest Classifier
importance = pd.Series(rf.feature_importances_, name='Gini')
feature_names = pd.Series(X2.columns, name='Feature')

df_features = pd.concat([feature_names, importance], axis=1, names = ['Feature', 'Gini'])
print(df_features)

from matplotlib.pyplot import rcParams
rcParams['figure.figsize'] = 10,15
df_sorted = df_features.sort_values('Gini', ascending = False)[:20]
ax=sns.barplot(x='Gini', y="Feature", data=df_sorted)
plt.xlabel('Feature Importance - predicted by RandomForest')
plt.show()

# NOTE: The program above is specifically tailored towards Highest Expression Scores simulation. For predicting under different scenarios, please follow the commented directions to update the program where necessary.
# NOTE: For ease of programming, Jupyter Notebook is the suggested platform for efficient data analysis and machine learning visualization.
